{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b3cdf-2c4c-4cf6-b08e-88813dda4828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438179b5-646b-413f-a40a-a0ca59757c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, glob, torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from src.models import CBOW\n",
    "from src.utils import train, compute_accuracy, set_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2ab96-dfa5-4a88-ba03-27f019ea8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 265\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = set_device(\"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda52a3d-c2fd-4f1c-90b3-f4b234f629ae",
   "metadata": {},
   "source": [
    "# Tokenization and creation of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dadfec-a34c-4791-8820-5b814814d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_EN = get_tokenizer(\"basic_english\")\n",
    "PATH_GENERATED = \"./generated_data/\"\n",
    "MIN_FREQ = 100\n",
    "DEBUGGING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5257b3-0b34-472f-961a-6b0220f39ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(datapath=\"./data/data_train/\", debug=DEBUGGING):\n",
    "    files = glob.glob(datapath + \"*.txt\")\n",
    "    if debug:\n",
    "        files = files[:1]\n",
    "\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        with open(f_name) as f:\n",
    "            lines += f.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4bf6b6-90b7-4f4b-a71f-9313aebc26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, tokenizer=TOKENIZER_EN):\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08402a7-e3c8-4528-a4c2-d2e183b173e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(lines, tokenizer=TOKENIZER_EN):\n",
    "    no_digits = \"\\w*[0-9]+\\w*\"\n",
    "    no_names = \"\\w*[A-Z]+\\w*\"\n",
    "    no_spaces = \"\\s+\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits, \" \", line)\n",
    "        line = re.sub(no_names, \" \", line)\n",
    "        line = re.sub(no_spaces, \" \", line)\n",
    "        yield tokenizer(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cc1a5-e687-41f9-8699-1fd49cce6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freqs(words, vocab):\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03b82e-fd51-4756-819d-38ef0191ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"]\n",
    "    )\n",
    "    vocab.append_token(\"i\")\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac64b1e-7ea9-42cb-a6f1-dbf2b502c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts\n",
    "# Load tokenized texts if they are generated\n",
    "# else, create it and save it\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\")\n",
    "    words_val = torch.load(PATH_GENERATED + \"words_val.pt\")\n",
    "    words_test = torch.load(PATH_GENERATED + \"words_test.pt\")\n",
    "else:\n",
    "    lines_book_train = read_files(\"./data/data_train/\")\n",
    "    lines_book_val = read_files(\"./data/data_val/\")\n",
    "    lines_book_test = read_files(\"./data/data_test/\")\n",
    "\n",
    "    words_train = tokenize(lines_book_train)\n",
    "    words_val = tokenize(lines_book_val)\n",
    "    words_test = tokenize(lines_book_test)\n",
    "\n",
    "    torch.save(words_train, PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val, PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test, PATH_GENERATED + \"words_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b4720-6b5b-41cc-ab7e-c34fbb658217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME)\n",
    "else:\n",
    "    vocab = create_vocabulary(lines_book_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40376d-b6ae-48ff-ad92-3135ccbf6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame([w for w in vocab.lookup_tokens(range(len(vocab)))])\n",
    "vocab_df.to_csv(PATH_GENERATED+\"vocab.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83448f96-53e3-4570-b735-4b138f2145f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset:   \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:         \", len(words_test))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words in the validation dataset:  \", len(set(words_val)))\n",
    "print(\"Number of distinct words in the test dataset:  \", len(set(words_test)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da381e7b-0587-4b5a-a61e-44f64efed6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_freqs(words_train, vocab)\n",
    "weights = 1 / freqs\n",
    "torch.save(weights, PATH_GENERATED + \"class_weights.pt\")\n",
    "# print(\n",
    "#     \"occurences:\\n\",\n",
    "#     [(f.item(), w) for (f, w) in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc6973-eff7-4dcb-a7a9-2fead183880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define targets\n",
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {vocab[w]: w for w in vocab.lookup_tokens(range(VOCAB_SIZE))}\n",
    "torch.save(MAP_TARGET, PATH_GENERATED + \"mapping.pt\")\n",
    "\n",
    "# context size for behind and after target\n",
    "CONTEXT_SIZE = 6\n",
    "\n",
    "# define context / target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bfb7d-39d0-4067-94a0-3803f5e6d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(text, vocab, context_size=CONTEXT_SIZE, map_target=MAP_TARGET):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "\n",
    "    n_text = len(text)\n",
    "    n_vocab = len(vocab)\n",
    "\n",
    "    if map_target is None:\n",
    "        map_target = {i: i for i in range(n_vocab)}\n",
    "\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(context_size, n_text - context_size):\n",
    "\n",
    "        t = txt[i]\n",
    "        # exclude <unk>(0) and/or punctuation(1) from targets\n",
    "        if map_target[t] in [\"<unk>\", \",\", \".\", \"(\", \")\", \"?\", \"!\"]:\n",
    "            pass\n",
    "        else:\n",
    "            # print(\"\\nindex: \", i)\n",
    "            # print(\"Context indices: \", i-context_size, i + context_size+1)\n",
    "            c = txt[i - context_size : i] + txt[i + 1 : i + context_size + 1]\n",
    "            # targets.append(map_target[t])\n",
    "            targets.append(t)\n",
    "            contexts.append(torch.tensor(c))\n",
    "\n",
    "    # contexts of shape (N_dataset, contexts_size)\n",
    "    # targets of shape (N_dataset)\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457e104-2459-4788-8b2d-f7bab9368fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe2230-8270-4dc6-bc0a-4599ccf36578",
   "metadata": {},
   "source": [
    "# Training embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dcbd66-5097-4d2a-adb5-8f6cea1ded54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\")\n",
    "data_val = load_dataset(words_val, vocab, \"data_val.pt\")\n",
    "data_test = load_dataset(words_test, vocab, \"data_test.pt\")\n",
    "\n",
    "print(f\"Context, target pairs in training set: {len(data_train)}\")\n",
    "print(f\"Context, target pairs in validation set: {len(data_val)}\")\n",
    "print(f\"Context, target pairs in test set: {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7927818-6255-42b3-8cc9-a5671cf0c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.load(\"generated_data/vocabulary.pt\")\n",
    "vocab_weights = torch.load(\"generated_data/class_weights.pt\")\n",
    "vocab_weights = vocab_weights.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398bddc-35c5-474e-ba85-79bf752e8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 1\n",
    "loss_fn = nn.NLLLoss(weight=vocab_weights)\n",
    "\n",
    "print(f\"-- Global Parameters --\")\n",
    "print(f\"{batch_size=}\")\n",
    "print(f\"{n_epochs=}\")\n",
    "\n",
    "parameter_search = [\n",
    "    # {\"lr\":0.001, \"embedding_dim\": 12},\n",
    "    {\"lr\":0.001, \"embedding_dim\": 16},\n",
    "    # {\"lr\":0.01, \"embedding_dim\": 12},\n",
    "    # {\"lr\":0.01, \"embedding_dim\": 16},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c62c1e-69d3-4628-9236-4cc3ccbe6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size=batch_size)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e32815-2409-4962-866b-dd73c74b0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_perf = []\n",
    "models = []\n",
    "\n",
    "for params in parameter_search:\n",
    "    print(\"\\n-- Training with following parameters --:\")\n",
    "    for name, val in params.items():\n",
    "        print(f\"{name}: {val}\")\n",
    "    torch.manual_seed(SEED)\n",
    "    # TODO: USE the same context size variable in notebook and embedding.py\n",
    "    model = CBOW(len(vocab), CONTEXT_SIZE, params[\"embedding_dim\"])\n",
    "    model.to(DEVICE)\n",
    "    optimizer = Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    \n",
    "    train_loss, val_loss, train_acc, val_acc = train(n_epochs, model, optimizer, loss_fn, train_loader, val_loader, DEVICE)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    val_perf.append(val_acc[-1])\n",
    "    models.append(model)\n",
    "    print(f\"Train accuracy: {train_acc[-1]*100:.3f}%\")\n",
    "    print(f\"Validation accuracy: {val_acc[-1]*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b21ff6-f1a2-4a5e-ae08-fae1809a00dd",
   "metadata": {},
   "source": [
    "# Embedding selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1f0c4-ce2f-4fe4-8e29-7d13bf141cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_index = val_perf.index(max(val_perf))\n",
    "chosen_model = models[chosen_index]\n",
    "torch.save(chosen_model.embedding, PATH_GENERATED+\"embedding_matrix.pt\")\n",
    "\n",
    "embedding_frame = pd.DataFrame(chosen_model.embedding.weight.to(\"cpu\").detach()).astype(\"float64\")\n",
    "embedding_frame.to_csv(PATH_GENERATED+\"embedding.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5ac58-6e28-4769-bd73-9a7e31ffb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_over_time(\n",
    "        train_perf: list[float],\n",
    "        val_perf: list[float],\n",
    "        title: str,\n",
    "        y_label: str,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Creates a plot of training and validation loss/performance over time.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(title)\n",
    "        ax.plot(train_perf, label=\"train\")\n",
    "        ax.plot(val_perf, label=\"val\")\n",
    "        ax.legend()\n",
    "\n",
    "        plt.ylabel(y_label)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ce5ff-d36c-4aed-b7e8-a8e5c0ca3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance_over_time(train_losses[chosen_index], val_losses[chosen_index], \"Training and Validation loss of chosen model\", \"loss\")\n",
    "plot_performance_over_time(train_accs[chosen_index], val_accs[chosen_index], \"Training and Validation accuracy of chosen model\", \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675e125-5a77-484c-b58c-e16ff0b0343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(data_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33db13-3a94-4e2b-b5f5-bf2f73849fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = MAP_TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eacff4-84ee-4e45-a80a-c6f2ac8cdda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1)\n",
    "embedding = chosen_model.embedding.weight.clone()\n",
    "\n",
    "print(\"-- 10 most similar words --\")\n",
    "words = [\"me\", \"white\", \"man\", \"have\", \"be\", \"child\", \"yes\", \"what\"]\n",
    "for word in words:\n",
    "    vocab_index = vocab[word]\n",
    "    similarity = cos(embedding[vocab_index].view(1, -1), embedding)\n",
    "    idx_ten = torch.topk(similarity, 11).indices\n",
    "    most_similar = [mapping[int(i)] for i in idx_ten][1:] #  Exclude similarity with itself\n",
    "    if vocab_index == 0:\n",
    "        print(f\"{word}({mapping[int(vocab_index)]}): {most_similar}\")\n",
    "    else:\n",
    "        print(f\"{word}: {most_similar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8ebd4-5c3b-4356-a307-26ca2d0b22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 1\n",
    "context, target = data_val[example_idx][0], data_val[example_idx][1]\n",
    "context = context.view(1, -1)\n",
    "chosen_model.eval()\n",
    "out = chosen_model(context.to(DEVICE))\n",
    "out = out.to(\"cpu\")\n",
    "\n",
    "print(\"Context indices: \", context)\n",
    "print(\"Context words: \", end=\" \")\n",
    "for idx in context[0]:\n",
    "    print(mapping[int(idx)], end=\" \")\n",
    "print()\n",
    "\n",
    "most_likely_idx = out.argmax()\n",
    "print(\"Target index: \", target)\n",
    "print(\"Predicted index: \", most_likely_idx)\n",
    "most_likely_word = mapping[int(most_likely_idx)]\n",
    "print(\"Target word: \", mapping[int(target)])\n",
    "print(\"Predicted word: \", most_likely_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
